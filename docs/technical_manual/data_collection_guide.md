# æ•°æ®é‡‡é›†æŒ‡å—

## ğŸ“¥ æ•°æ®é‡‡é›†å®Œæ•´æµç¨‹

### ğŸš€ å¿«é€Ÿå¼€å§‹

```bash
# 1. æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
venv\Scripts\activate

# 2. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 3. è¿è¡Œæ•°æ®é‡‡é›†
python scripts/main.py
```

### ğŸ”§ é…ç½®å‚æ•°

#### ç¼–è¾‘é…ç½®æ–‡ä»¶
æ–‡ä»¶ä½ç½®: `src/utils/config.py`

```python
# äº¤æ˜“å¯¹è®¾ç½®
SYMBOL = "ETHUSDT"

# æ—¶é—´é—´éš” (æ”¯æŒ: 1m, 5m, 15m, 30m, 1h, 4h, 1d)
INTERVALS = ["1h", "4h"]

# HMAå‘¨æœŸ (æ¨è: 45)
HMA_PERIOD = 45

# å†å²æ•°æ®å¹´æ•°
YEARS_BACK = 3

# å¸å®‰APIé…ç½®
BINANCE_BASE_URL = "https://api.binance.com/api/v3"
BINANCE_KLINES_ENDPOINT = "/klines"

# è¯·æ±‚é™åˆ¶
MAX_RETRIES = 3
REQUEST_DELAY = 0.1  # ç§’
```

### ğŸ“Š æ•°æ®é‡‡é›†æµç¨‹

#### æ­¥éª¤1: åˆå§‹åŒ–æ•°æ®é‡‡é›†å™¨
```python
from src.collectors.data_collector import DataCollector

collector = DataCollector()
```

#### æ­¥éª¤2: é‡‡é›†å†å²æ•°æ®
```python
# é‡‡é›†4å°æ—¶æ•°æ®ï¼Œè¿‡å»3å¹´
raw_data = collector.collect_historical_data("ETHUSDT", "4h", 3)
```

#### æ­¥éª¤3: æ•°æ®éªŒè¯
```python
# æ£€æŸ¥æ•°æ®è´¨é‡
print(f"æ•°æ®å½¢çŠ¶: {raw_data.shape}")
print(f"æ—¶é—´èŒƒå›´: {raw_data.index.min()} åˆ° {raw_data.index.max()}")
print(f"ç¼ºå¤±å€¼: {raw_data.isnull().sum().sum()}")
```

### ğŸ—‚ï¸ æ•°æ®å­˜å‚¨

#### åŸå§‹æ•°æ®æ–‡ä»¶
- **ä½ç½®**: `src/utils/data/ETHUSDT_4h_raw_*.parquet`
- **æ ¼å¼**: Parquet (é«˜æ•ˆå‹ç¼©)
- **å†…å®¹**: å¸å®‰APIåŸå§‹Kçº¿æ•°æ®

#### å¤„ç†åæ•°æ®æ–‡ä»¶
- **ä½ç½®**: `src/utils/data/ETHUSDT_4h_processed_*.parquet`
- **æ ¼å¼**: Parquet
- **å†…å®¹**: åŒ…å«HMAç­‰æŠ€æœ¯æŒ‡æ ‡çš„æ•°æ®

### ğŸ“‹ æ•°æ®æ ¼å¼è¯´æ˜

#### åŸå§‹Kçº¿æ•°æ®åˆ—
```python
columns = [
    'open_time',      # å¼€ç›˜æ—¶é—´ (UTC)
    'open',           # å¼€ç›˜ä»·
    'high',           # æœ€é«˜ä»·
    'low',            # æœ€ä½ä»·
    'close',          # æ”¶ç›˜ä»·
    'volume',         # æˆäº¤é‡
    'close_time',     # æ”¶ç›˜æ—¶é—´
    'quote_asset_volume',  # æˆäº¤é¢
    'trades_count',   # æˆäº¤ç¬”æ•°
    'taker_buy_base_asset_volume',    # ä¸»åŠ¨ä¹°å…¥æˆäº¤é‡
    'taker_buy_quote_asset_volume',   # ä¸»åŠ¨ä¹°å…¥æˆäº¤é¢
    'ignore'          # å¿½ç•¥å­—æ®µ
]
```

#### å¤„ç†åæ•°æ®é¢å¤–åˆ—
```python
additional_columns = [
    f'HMA_{HMA_PERIOD}',  # Hullç§»åŠ¨å¹³å‡
    'HMA_slope',          # HMAæ–œç‡
    'SMA_20',             # 20æœŸç®€å•ç§»åŠ¨å¹³å‡
    'SMA_50',             # 50æœŸç®€å•ç§»åŠ¨å¹³å‡
    'price_change',       # ä»·æ ¼å˜åŒ–ç‡
    'price_change_abs',   # ç»å¯¹ä»·æ ¼å˜åŒ–
    'volatility',         # æ³¢åŠ¨ç‡
    'hma_deviation'       # HMAåç¦»åº¦
]
```

### ğŸ”„ æ•°æ®é‡‡é›†APIè¯¦è§£

#### DataCollectorç±»æ–¹æ³•

```python
class DataCollector:
    def __init__(self):
        """åˆå§‹åŒ–æ•°æ®é‡‡é›†å™¨"""
        
    def get_klines_data(self, symbol, interval, start_time, end_time):
        """è·å–Kçº¿æ•°æ®"""
        # å‚æ•°:
        #   symbol: äº¤æ˜“å¯¹ (å¦‚ 'ETHUSDT')
        #   interval: æ—¶é—´é—´éš” (å¦‚ '4h')
        #   start_time: å¼€å§‹æ—¶é—´æˆ³ (æ¯«ç§’)
        #   end_time: ç»“æŸæ—¶é—´æˆ³ (æ¯«ç§’)
        # è¿”å›: Kçº¿æ•°æ®åˆ—è¡¨
        
    def collect_historical_data(self, symbol, interval, years_back):
        """é‡‡é›†å†å²æ•°æ®"""
        # å‚æ•°:
        #   symbol: äº¤æ˜“å¯¹
        #   interval: æ—¶é—´é—´éš”
        #   years_back: å†å²å¹´æ•°
        # è¿”å›: DataFrame
        
    def _get_interval_ms(self, interval):
        """æ—¶é—´é—´éš”è½¬æ¯«ç§’"""
        # æ”¯æŒ: 1m, 5m, 15m, 30m, 1h, 4h, 1d
        
    def _convert_to_dataframe(self, klines_data):
        """è½¬æ¢æ•°æ®æ ¼å¼"""
        # å°†å¸å®‰APIæ•°æ®è½¬æ¢ä¸ºDataFrame
```

### âš ï¸ æ³¨æ„äº‹é¡¹

#### 1. APIé™åˆ¶
- å¸å®‰APIæœ‰è¯·æ±‚é¢‘ç‡é™åˆ¶
- å•æ¬¡æœ€å¤šè¿”å›1000æ¡æ•°æ®
- å»ºè®®è¯·æ±‚é—´éš”0.1ç§’

#### 2. æ•°æ®è´¨é‡
- æ£€æŸ¥æ—¶é—´è¿ç»­æ€§
- éªŒè¯ä»·æ ¼æ•°æ®åˆç†æ€§
- å¤„ç†ç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼

#### 3. å­˜å‚¨ç®¡ç†
- Parquetæ ¼å¼é«˜æ•ˆå‹ç¼©
- å®šæœŸæ¸…ç†æ—§æ•°æ®æ–‡ä»¶
- å¤‡ä»½é‡è¦æ•°æ®

### ğŸ› å¸¸è§é—®é¢˜

#### é—®é¢˜1: ç½‘ç»œè¿æ¥å¤±è´¥
```bash
# é”™è¯¯: requests.exceptions.ConnectionError
# è§£å†³: æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œä»£ç†è®¾ç½®
```

#### é—®é¢˜2: APIé™åˆ¶
```bash
# é”™è¯¯: 429 Too Many Requests
# è§£å†³: å¢åŠ è¯·æ±‚é—´éš”æ—¶é—´
```

#### é—®é¢˜3: æ•°æ®ä¸å®Œæ•´
```bash
# é”™è¯¯: æ•°æ®ç¼ºå¤±æˆ–æ—¶é—´ä¸è¿ç»­
# è§£å†³: é‡æ–°é‡‡é›†æˆ–è¡¥å……æ•°æ®
```

### ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

#### 1. æ‰¹é‡å¤„ç†
```python
# åˆ†æ‰¹é‡‡é›†å¤§é‡æ•°æ®
batch_size = 1000
for i in range(0, total_records, batch_size):
    batch_data = collector.get_klines_data(...)
    # å¤„ç†æ‰¹æ¬¡æ•°æ®
```

#### 2. å¹¶è¡Œå¤„ç†
```python
# ä½¿ç”¨å¤šçº¿ç¨‹åŠ é€Ÿ
from concurrent.futures import ThreadPoolExecutor

with ThreadPoolExecutor(max_workers=4) as executor:
    futures = [executor.submit(collect_data, params) for params in param_list]
```

#### 3. å†…å­˜ç®¡ç†
```python
# åˆ†å—å¤„ç†å¤§æ•°æ®
chunk_size = 10000
for chunk in pd.read_parquet(file, chunksize=chunk_size):
    # å¤„ç†æ•°æ®å—
```

### ğŸ” æ•°æ®éªŒè¯

#### å®Œæ•´æ€§æ£€æŸ¥
```python
def validate_data(df):
    """éªŒè¯æ•°æ®å®Œæ•´æ€§"""
    checks = {
        'shape': df.shape,
        'time_range': (df.index.min(), df.index.max()),
        'missing_values': df.isnull().sum().sum(),
        'duplicates': df.index.duplicated().sum(),
        'price_range': (df['low'].min(), df['high'].max())
    }
    return checks
```

#### æ•°æ®è´¨é‡æŠ¥å‘Š
```python
def generate_quality_report(df):
    """ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š"""
    report = {
        'total_records': len(df),
        'time_span': (df.index.max() - df.index.min()).days,
        'missing_rate': df.isnull().sum().sum() / (len(df) * len(df.columns)),
        'price_anomalies': detect_price_anomalies(df),
        'volume_anomalies': detect_volume_anomalies(df)
    }
    return report
```

---

*æœ€åæ›´æ–°: 2025-09-27*
*ç‰ˆæœ¬: v1.0.0*
